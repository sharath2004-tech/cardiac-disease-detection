{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharath2004-tech/cardiac-disease-detection/blob/main/cardiac%20disease%20detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1p2Jeth8dGHR",
        "outputId": "0836c48c-2269-4998-877e-5defed3f1d1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting wfdb\n",
            "  Downloading wfdb-4.3.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: aiohttp>=3.10.11 in /usr/local/lib/python3.12/dist-packages (from wfdb) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2025.3.0)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from wfdb) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2.0.2)\n",
            "Collecting pandas>=2.2.3 (from wfdb)\n",
            "  Downloading pandas-3.0.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (1.16.3)\n",
            "Requirement already satisfied: soundfile>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (0.13.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.22.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (26.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (2026.1.4)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.10.0->wfdb) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp>=3.10.11->wfdb) (4.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb) (3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->wfdb) (1.17.0)\n",
            "Downloading wfdb-4.3.1-py3-none-any.whl (163 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.9/163.9 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-3.0.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (10.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m113.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pandas, wfdb\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 3.0.1 which is incompatible.\n",
            "dask-cudf-cu12 25.10.0 requires pandas<2.4.0dev0,>=2.0, but you have pandas 3.0.1 which is incompatible.\n",
            "gradio 5.50.0 requires pandas<3.0,>=1.0, but you have pandas 3.0.1 which is incompatible.\n",
            "cudf-cu12 25.10.0 requires pandas<2.4.0dev0,>=2.0, but you have pandas 3.0.1 which is incompatible.\n",
            "bqplot 0.12.45 requires pandas<3.0.0,>=1.0.0, but you have pandas 3.0.1 which is incompatible.\n",
            "db-dtypes 1.5.0 requires pandas<3.0.0,>=1.5.3, but you have pandas 3.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-3.0.1 wfdb-4.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install wfdb\n",
        "\n",
        "\n",
        "import wfdb\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I73C5-hjdGHS"
      },
      "outputs": [],
      "source": [
        "wfdb.dl_database(\n",
        "    'mitdb',\n",
        "    dl_dir='mitdb',\n",
        "    keep_subdirs=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_itPsjMdGHS"
      },
      "outputs": [],
      "source": [
        "import wfdb\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "record = wfdb.rdrecord('mitdb/100')\n",
        "annotation = wfdb.rdann('mitdb/100', 'atr')\n",
        "\n",
        "signal = record.p_signal[:,0]   # use first channel\n",
        "r_peaks = annotation.sample\n",
        "labels = annotation.symbol\n",
        "\n",
        "print(\"Signal shape:\", signal.shape)\n",
        "print(\"Total beats:\", len(r_peaks))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfJSmA0qdGHT"
      },
      "outputs": [],
      "source": [
        "beats = []\n",
        "beat_labels = []\n",
        "\n",
        "for i in range(len(r_peaks)):\n",
        "    start = r_peaks[i] - 90\n",
        "    end = r_peaks[i] + 90\n",
        "\n",
        "    if start > 0 and end < len(signal):\n",
        "        beat = signal[start:end]\n",
        "        beats.append(beat)\n",
        "        beat_labels.append(labels[i])\n",
        "\n",
        "beats = np.array(beats)\n",
        "print(\"Beats shape:\", beats.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwSiNcnrdGHT"
      },
      "outputs": [],
      "source": [
        "AAMI_map = {\n",
        "    'N':'N','L':'N','R':'N','e':'N','j':'N',\n",
        "    'A':'S','a':'S','J':'S','S':'S',\n",
        "    'V':'V','E':'V',\n",
        "    'F':'F'\n",
        "}\n",
        "\n",
        "class_to_int = {'N':0,'S':1,'V':2,'F':3}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gy0TPZhldGHU"
      },
      "outputs": [],
      "source": [
        "# Standard DS1 (Train) and DS2 (Test) split used in research\n",
        "\n",
        "train_records = [\n",
        "    '101','106','108','109','112','114','115','116',\n",
        "    '118','119','122','124','201','203','205','207',\n",
        "    '208','209','215','220','223','230'\n",
        "]\n",
        "\n",
        "test_records = [\n",
        "    '100','103','105','111','113','117','121','123',\n",
        "    '200','202','210','212','213','214','219','221',\n",
        "    '222','228','231','232','233','234'\n",
        "]\n",
        "\n",
        "print(\"Train records:\", len(train_records))\n",
        "print(\"Test records:\", len(test_records))\n",
        "\n",
        "# Safety check (no leakage)\n",
        "print(\"Overlap:\", set(train_records) & set(test_records))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3CUjCATdGHU"
      },
      "outputs": [],
      "source": [
        "def extract_beats(record_list):\n",
        "    beats = []\n",
        "    labels = []\n",
        "\n",
        "    for rec in record_list:\n",
        "        print(f\"Processing record {rec}...\")\n",
        "\n",
        "        record = wfdb.rdrecord(f'mitdb/{rec}')\n",
        "        annotation = wfdb.rdann(f'mitdb/{rec}', 'atr')\n",
        "\n",
        "        signal = record.p_signal[:, 0]   # Use first channel\n",
        "        r_peaks = annotation.sample\n",
        "        symbols = annotation.symbol\n",
        "\n",
        "        for i in range(len(r_peaks)):\n",
        "            start = r_peaks[i] - 90\n",
        "            end = r_peaks[i] + 90\n",
        "\n",
        "            if start > 0 and end < len(signal):\n",
        "                label = symbols[i]\n",
        "\n",
        "                if label in AAMI_map:\n",
        "                    beat = signal[start:end]\n",
        "\n",
        "                    # Per-beat normalization\n",
        "                    mean = np.mean(beat)\n",
        "                    std = np.std(beat)\n",
        "                    beat = (beat - mean) / (std + 1e-8)\n",
        "\n",
        "                    beats.append(beat)\n",
        "                    labels.append(class_to_int[AAMI_map[label]])\n",
        "\n",
        "    return np.array(beats), np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5erwntlRdGHV"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = extract_beats(train_records)\n",
        "X_test, y_test = extract_beats(test_records)\n",
        "\n",
        "print(\"Train shape:\", X_train.shape)\n",
        "print(\"Test shape:\", X_test.shape)\n",
        "\n",
        "import numpy as np\n",
        "print(\"Unique classes in train:\", np.unique(y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jS3lxlAqdGHV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "\n",
        "class ECGDataset(Dataset):\n",
        "    def __init__(self, X, y, augment=False):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32).unsqueeze(1)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        beat = self.X[idx].clone()\n",
        "\n",
        "        if self.augment:\n",
        "            # Gaussian noise\n",
        "            if random.random() > 0.5:\n",
        "                noise = torch.randn_like(beat) * 0.02\n",
        "                beat = beat + noise\n",
        "\n",
        "            # Amplitude scaling\n",
        "            if random.random() > 0.5:\n",
        "                scale = random.uniform(0.9, 1.1)\n",
        "                beat = beat * scale\n",
        "\n",
        "            # Time shifting (circular shift)\n",
        "            if random.random() > 0.5:\n",
        "                shift = random.randint(-10, 10)\n",
        "                beat = torch.roll(beat, shift, dims=-1)\n",
        "\n",
        "            # Baseline wander\n",
        "            if random.random() > 0.5:\n",
        "                baseline = torch.sin(torch.linspace(0, 4*3.14159, beat.size(-1))) * 0.05\n",
        "                beat = beat + baseline.unsqueeze(0)\n",
        "\n",
        "        return beat, self.y[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_HDFWk0dGHV"
      },
      "outputs": [],
      "source": [
        "train_dataset = ECGDataset(X_train, y_train, augment=True)\n",
        "test_dataset = ECGDataset(X_test, y_test, augment=False)\n",
        "\n",
        "# Smaller batch size for better generalization\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzljxzgEdGHW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "\n",
        "for u, c in zip(unique, counts):\n",
        "    print(f\"Class {u}: {c}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtSY8J4VdGHW"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "\n",
        "alpha = torch.tensor(class_weights, dtype=torch.float32)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "alpha = alpha.to(device)\n",
        "\n",
        "print(\"Class weights:\", alpha)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCmK-yrCdGHW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=5, padding=2)\n",
        "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv1d(in_channels, out_channels, kernel_size=1),\n",
        "                nn.BatchNorm1d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.shortcut(x)\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.dropout(out)\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += residual\n",
        "        return F.relu(out)\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.query = nn.Conv1d(channels, channels // 8, 1)\n",
        "        self.key = nn.Conv1d(channels, channels // 8, 1)\n",
        "        self.value = nn.Conv1d(channels, channels, 1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, channels, length = x.size()\n",
        "\n",
        "        q = self.query(x).view(batch, -1, length).permute(0, 2, 1)\n",
        "        k = self.key(x).view(batch, -1, length)\n",
        "        v = self.value(x).view(batch, -1, length)\n",
        "\n",
        "        attention = torch.bmm(q, k)\n",
        "        attention = F.softmax(attention / math.sqrt(channels // 8), dim=-1)\n",
        "\n",
        "        out = torch.bmm(v, attention.permute(0, 2, 1))\n",
        "        out = out.view(batch, channels, length)\n",
        "\n",
        "        return self.gamma * out + x\n",
        "\n",
        "\n",
        "class ECGResNet(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv1d(1, 32, kernel_size=7, padding=3),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2)\n",
        "        )\n",
        "\n",
        "        self.layer2 = ResidualBlock(32, 64, dropout=0.2)\n",
        "        self.layer3 = ResidualBlock(64, 128, dropout=0.3)\n",
        "        self.layer4 = ResidualBlock(128, 256, dropout=0.3)\n",
        "        self.layer5 = ResidualBlock(256, 256, dropout=0.4)\n",
        "\n",
        "        self.attention = SelfAttention(256)\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.global_max_pool = nn.AdaptiveMaxPool1d(1)\n",
        "\n",
        "        self.fc1 = nn.Linear(512, 256)\n",
        "        self.bn_fc = nn.BatchNorm1d(256)\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.dropout2 = nn.Dropout(0.4)\n",
        "\n",
        "        self.fc3 = nn.Linear(128, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.layer5(x)\n",
        "\n",
        "        x = self.attention(x)\n",
        "\n",
        "        # Concatenate avg and max pooling\n",
        "        avg_pool = self.global_pool(x).squeeze(-1)\n",
        "        max_pool = self.global_max_pool(x).squeeze(-1)\n",
        "        x = torch.cat([avg_pool, max_pool], dim=1)\n",
        "\n",
        "        x = F.relu(self.bn_fc(self.fc1(x)))\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfBaNsxVdGHX"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = ECGResNet().to(device)\n",
        "\n",
        "# Use AdamW with weight decay for better regularization\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "# Cosine annealing with warmup\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "    optimizer, T_0=10, T_mult=2, eta_min=1e-6\n",
        ")\n",
        "\n",
        "print(\"Using device:\", device)\n",
        "print(\"Model parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtZR28AYdGHX"
      },
      "outputs": [],
      "source": [
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    def __init__(self, smoothing=0.1, weight=None):\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "        self.weight = weight\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        n_class = pred.size(1)\n",
        "        one_hot = torch.zeros_like(pred).scatter(1, target.view(-1, 1), 1)\n",
        "        one_hot = one_hot * (1 - self.smoothing) + self.smoothing / n_class\n",
        "        log_prb = F.log_softmax(pred, dim=1)\n",
        "\n",
        "        if self.weight is not None:\n",
        "            loss = -(one_hot * log_prb).sum(dim=1)\n",
        "            loss = (loss * self.weight[target]).mean()\n",
        "        else:\n",
        "            loss = -(one_hot * log_prb).sum(dim=1).mean()\n",
        "        return loss\n",
        "\n",
        "criterion = LabelSmoothingCrossEntropy(smoothing=0.1, weight=alpha)\n",
        "\n",
        "def train_model(model, train_loader, test_loader, criterion, optimizer, scheduler, epochs=50):\n",
        "    best_val_acc = 0.0\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step(epoch + total / len(train_loader.dataset))\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "\n",
        "        train_acc = 100 * correct / total\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        val_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in test_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_total += targets.size(0)\n",
        "                val_correct += (predicted == targets).sum().item()\n",
        "\n",
        "        val_acc = 100 * val_correct / val_total\n",
        "\n",
        "        # Track best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}] \"\n",
        "              f\"Train Acc: {train_acc:.2f}% \"\n",
        "              f\"Val Acc: {val_acc:.2f}% \"\n",
        "              f\"Best Val: {best_val_acc:.2f}% \"\n",
        "              f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if patience_counter >= 15:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "    print(f\"\\nBest Validation Accuracy: {best_val_acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6OuUlzRdGHX"
      },
      "outputs": [],
      "source": [
        "train_model(model, train_loader, test_loader, criterion, optimizer, scheduler, epochs=50)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
