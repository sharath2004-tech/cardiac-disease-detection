{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPaKo4msKBc5jQZLXlBRpfB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharath2004-tech/cardiac-disease-detection/blob/main/cardiac%20disease%20detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgI42isP1YUY",
        "outputId": "ddbabc0a-46ea-4563-c087-0ba8e11867d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wfdb in /usr/local/lib/python3.12/dist-packages (4.3.1)\n",
            "Requirement already satisfied: aiohttp>=3.10.11 in /usr/local/lib/python3.12/dist-packages (from wfdb) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2025.3.0)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from wfdb) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2.0.2)\n",
            "Requirement already satisfied: pandas>=2.2.3 in /usr/local/lib/python3.12/dist-packages (from wfdb) (3.0.1)\n",
            "Requirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (1.16.3)\n",
            "Requirement already satisfied: soundfile>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (0.13.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.22.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (26.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (2026.1.4)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.10.0->wfdb) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp>=3.10.11->wfdb) (4.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb) (3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->wfdb) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install wfdb\n",
        "\n",
        "\n",
        "import wfdb\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "wfdb.dl_database(\n",
        "    'mitdb',\n",
        "    dl_dir='mitdb',\n",
        "    keep_subdirs=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OquLi3pk1zrf",
        "outputId": "8ce6801e-0d97-4949-f3ad-e92688439250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating record list for: 100\n",
            "Generating record list for: 101\n",
            "Generating record list for: 102\n",
            "Generating record list for: 103\n",
            "Generating record list for: 104\n",
            "Generating record list for: 105\n",
            "Generating record list for: 106\n",
            "Generating record list for: 107\n",
            "Generating record list for: 108\n",
            "Generating record list for: 109\n",
            "Generating record list for: 111\n",
            "Generating record list for: 112\n",
            "Generating record list for: 113\n",
            "Generating record list for: 114\n",
            "Generating record list for: 115\n",
            "Generating record list for: 116\n",
            "Generating record list for: 117\n",
            "Generating record list for: 118\n",
            "Generating record list for: 119\n",
            "Generating record list for: 121\n",
            "Generating record list for: 122\n",
            "Generating record list for: 123\n",
            "Generating record list for: 124\n",
            "Generating record list for: 200\n",
            "Generating record list for: 201\n",
            "Generating record list for: 202\n",
            "Generating record list for: 203\n",
            "Generating record list for: 205\n",
            "Generating record list for: 207\n",
            "Generating record list for: 208\n",
            "Generating record list for: 209\n",
            "Generating record list for: 210\n",
            "Generating record list for: 212\n",
            "Generating record list for: 213\n",
            "Generating record list for: 214\n",
            "Generating record list for: 215\n",
            "Generating record list for: 217\n",
            "Generating record list for: 219\n",
            "Generating record list for: 220\n",
            "Generating record list for: 221\n",
            "Generating record list for: 222\n",
            "Generating record list for: 223\n",
            "Generating record list for: 228\n",
            "Generating record list for: 230\n",
            "Generating record list for: 231\n",
            "Generating record list for: 232\n",
            "Generating record list for: 233\n",
            "Generating record list for: 234\n",
            "Generating list of all files for: 100\n",
            "Generating list of all files for: 101\n",
            "Generating list of all files for: 102\n",
            "Generating list of all files for: 103\n",
            "Generating list of all files for: 104\n",
            "Generating list of all files for: 105\n",
            "Generating list of all files for: 106\n",
            "Generating list of all files for: 107\n",
            "Generating list of all files for: 108\n",
            "Generating list of all files for: 109\n",
            "Generating list of all files for: 111\n",
            "Generating list of all files for: 112\n",
            "Generating list of all files for: 113\n",
            "Generating list of all files for: 114\n",
            "Generating list of all files for: 115\n",
            "Generating list of all files for: 116\n",
            "Generating list of all files for: 117\n",
            "Generating list of all files for: 118\n",
            "Generating list of all files for: 119\n",
            "Generating list of all files for: 121\n",
            "Generating list of all files for: 122\n",
            "Generating list of all files for: 123\n",
            "Generating list of all files for: 124\n",
            "Generating list of all files for: 200\n",
            "Generating list of all files for: 201\n",
            "Generating list of all files for: 202\n",
            "Generating list of all files for: 203\n",
            "Generating list of all files for: 205\n",
            "Generating list of all files for: 207\n",
            "Generating list of all files for: 208\n",
            "Generating list of all files for: 209\n",
            "Generating list of all files for: 210\n",
            "Generating list of all files for: 212\n",
            "Generating list of all files for: 213\n",
            "Generating list of all files for: 214\n",
            "Generating list of all files for: 215\n",
            "Generating list of all files for: 217\n",
            "Generating list of all files for: 219\n",
            "Generating list of all files for: 220\n",
            "Generating list of all files for: 221\n",
            "Generating list of all files for: 222\n",
            "Generating list of all files for: 223\n",
            "Generating list of all files for: 228\n",
            "Generating list of all files for: 230\n",
            "Generating list of all files for: 231\n",
            "Generating list of all files for: 232\n",
            "Generating list of all files for: 233\n",
            "Generating list of all files for: 234\n",
            "Downloading files...\n",
            "Finished downloading files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wfdb\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "record = wfdb.rdrecord('mitdb/100')\n",
        "annotation = wfdb.rdann('mitdb/100', 'atr')\n",
        "\n",
        "signal = record.p_signal[:,0]   # use first channel\n",
        "r_peaks = annotation.sample\n",
        "labels = annotation.symbol\n",
        "\n",
        "print(\"Signal shape:\", signal.shape)\n",
        "print(\"Total beats:\", len(r_peaks))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bF1UAJQj2_ck",
        "outputId": "96ddc9be-f3c8-4625-bce0-c0beabdba130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Signal shape: (650000,)\n",
            "Total beats: 2274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "beats = []\n",
        "beat_labels = []\n",
        "\n",
        "for i in range(len(r_peaks)):\n",
        "    start = r_peaks[i] - 90\n",
        "    end = r_peaks[i] + 90\n",
        "\n",
        "    if start > 0 and end < len(signal):\n",
        "        beat = signal[start:end]\n",
        "        beats.append(beat)\n",
        "        beat_labels.append(labels[i])\n",
        "\n",
        "beats = np.array(beats)\n",
        "print(\"Beats shape:\", beats.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Fwx-2oH3o-3",
        "outputId": "ca446490-c987-4a24-8165-762ae2aee5e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beats shape: (2271, 180)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AAMI_map = {\n",
        "    'N':'N','L':'N','R':'N','e':'N','j':'N',\n",
        "    'A':'S','a':'S','J':'S','S':'S',\n",
        "    'V':'V','E':'V',\n",
        "    'F':'F'\n",
        "}\n",
        "\n",
        "class_to_int = {'N':0,'S':1,'V':2,'F':3}"
      ],
      "metadata": {
        "id": "Qeduxmu46WDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard DS1 (Train) and DS2 (Test) split used in research\n",
        "\n",
        "train_records = [\n",
        "    '101','106','108','109','112','114','115','116',\n",
        "    '118','119','122','124','201','203','205','207',\n",
        "    '208','209','215','220','223','230'\n",
        "]\n",
        "\n",
        "test_records = [\n",
        "    '100','103','105','111','113','117','121','123',\n",
        "    '200','202','210','212','213','214','219','221',\n",
        "    '222','228','231','232','233','234'\n",
        "]\n",
        "\n",
        "print(\"Train records:\", len(train_records))\n",
        "print(\"Test records:\", len(test_records))\n",
        "\n",
        "# Safety check (no leakage)\n",
        "print(\"Overlap:\", set(train_records) & set(test_records))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKFp5fIhXmMf",
        "outputId": "efe05e56-99b2-4b2a-d33c-2466a2d89cf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train records: 22\n",
            "Test records: 22\n",
            "Overlap: set()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_beats(record_list):\n",
        "    beats = []\n",
        "    labels = []\n",
        "\n",
        "    for rec in record_list:\n",
        "        print(f\"Processing record {rec}...\")\n",
        "\n",
        "        record = wfdb.rdrecord(f'mitdb/{rec}')\n",
        "        annotation = wfdb.rdann(f'mitdb/{rec}', 'atr')\n",
        "\n",
        "        signal = record.p_signal[:, 0]   # Use first channel\n",
        "        r_peaks = annotation.sample\n",
        "        symbols = annotation.symbol\n",
        "\n",
        "        for i in range(len(r_peaks)):\n",
        "            start = r_peaks[i] - 90\n",
        "            end = r_peaks[i] + 90\n",
        "\n",
        "            if start > 0 and end < len(signal):\n",
        "                label = symbols[i]\n",
        "\n",
        "                if label in AAMI_map:\n",
        "                    beat = signal[start:end]\n",
        "\n",
        "                    # Per-beat normalization\n",
        "                    mean = np.mean(beat)\n",
        "                    std = np.std(beat)\n",
        "                    beat = (beat - mean) / (std + 1e-8)\n",
        "\n",
        "                    beats.append(beat)\n",
        "                    labels.append(class_to_int[AAMI_map[label]])\n",
        "\n",
        "    return np.array(beats), np.array(labels)"
      ],
      "metadata": {
        "id": "n6Qwb46w8QTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z6sNKRDGX_Oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = extract_beats(train_records)\n",
        "X_test, y_test = extract_beats(test_records)\n",
        "\n",
        "print(\"Train shape:\", X_train.shape)\n",
        "print(\"Test shape:\", X_test.shape)\n",
        "\n",
        "import numpy as np\n",
        "print(\"Unique classes in train:\", np.unique(y_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HC2nYV5eXV9-",
        "outputId": "343f4b7e-156b-4b7e-9246-7e124cdcffe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing record 101...\n",
            "Processing record 106...\n",
            "Processing record 108...\n",
            "Processing record 109...\n",
            "Processing record 112...\n",
            "Processing record 114...\n",
            "Processing record 115...\n",
            "Processing record 116...\n",
            "Processing record 118...\n",
            "Processing record 119...\n",
            "Processing record 122...\n",
            "Processing record 124...\n",
            "Processing record 201...\n",
            "Processing record 203...\n",
            "Processing record 205...\n",
            "Processing record 207...\n",
            "Processing record 208...\n",
            "Processing record 209...\n",
            "Processing record 215...\n",
            "Processing record 220...\n",
            "Processing record 223...\n",
            "Processing record 230...\n",
            "Processing record 100...\n",
            "Processing record 103...\n",
            "Processing record 105...\n",
            "Processing record 111...\n",
            "Processing record 113...\n",
            "Processing record 117...\n",
            "Processing record 121...\n",
            "Processing record 123...\n",
            "Processing record 200...\n",
            "Processing record 202...\n",
            "Processing record 210...\n",
            "Processing record 212...\n",
            "Processing record 213...\n",
            "Processing record 214...\n",
            "Processing record 219...\n",
            "Processing record 221...\n",
            "Processing record 222...\n",
            "Processing record 228...\n",
            "Processing record 231...\n",
            "Processing record 232...\n",
            "Processing record 233...\n",
            "Processing record 234...\n",
            "Train shape: (51002, 180)\n",
            "Test shape: (49691, 180)\n",
            "Unique classes in train: [0 1 2 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "class ECGDataset(Dataset):\n",
        "    def __init__(self, X, y, augment=False):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32).unsqueeze(1)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        beat = self.X[idx]\n",
        "\n",
        "        if self.augment:\n",
        "            noise = torch.randn_like(beat) * 0.01\n",
        "            beat = beat + noise\n",
        "\n",
        "        return beat, self.y[idx]"
      ],
      "metadata": {
        "id": "uX3YYtQj8hYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ECGDataset(X_train, y_train, augment=True)\n",
        "test_dataset = ECGDataset(X_test, y_test, augment=False)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
      ],
      "metadata": {
        "id": "VFY3RMBn8uRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "\n",
        "for u, c in zip(unique, counts):\n",
        "    print(f\"Class {u}: {c}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rHHP7mP81mT",
        "outputId": "dbce0d6e-cbb4-4986-82b5-858f48515537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 0: 45856\n",
            "Class 1: 944\n",
            "Class 2: 3788\n",
            "Class 3: 414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=None, gamma=2):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.ce = nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = self.ce(inputs, targets)\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
        "\n",
        "        if self.alpha is not None:\n",
        "            at = self.alpha[targets]\n",
        "            focal_loss = at * focal_loss\n",
        "\n",
        "        return focal_loss.mean()"
      ],
      "metadata": {
        "id": "lKRkkR0Y8-aC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "\n",
        "alpha = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=alpha)\n",
        "\n",
        "print(\"Class weights:\", alpha)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnetEuEx9KXK",
        "outputId": "430a6382-f082-4a15-bd8a-c6ac75abbf85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class weights: tensor([ 0.2781, 13.5069,  3.3660, 30.7983], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=5, padding=2)\n",
        "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv1d(in_channels, out_channels, kernel_size=1),\n",
        "                nn.BatchNorm1d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.shortcut(x)\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += residual\n",
        "        return F.relu(out)\n",
        "\n",
        "\n",
        "class ECGResNet(nn.Module):\n",
        "    def __init__(self, num_classes=5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv1d(1, 32, kernel_size=7, padding=3),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2)\n",
        "        )\n",
        "\n",
        "        self.layer2 = ResidualBlock(32, 64)\n",
        "        self.layer3 = ResidualBlock(64, 128)\n",
        "        self.layer4 = ResidualBlock(128, 256)   # NEW deeper block\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        self.fc1 = nn.Linear(256, 128)          # updated input size\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(128, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)   # NEW\n",
        "\n",
        "        x = self.global_pool(x).squeeze(-1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "S9dbIy8D9kRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = ECGResNet().to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)  # LOWER LR\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='max', patience=3, factor=0.5\n",
        ")\n",
        "\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdJffgB29lzK",
        "outputId": "9bd598ae-4266-4858-baa7-ec967eb637b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, test_loader, criterion, optimizer, scheduler, epochs=20):\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # ðŸ”¥ Gradient Clipping (NEW)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "\n",
        "        train_acc = 100 * correct / total\n",
        "\n",
        "        # ===== Validation =====\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in test_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_total += targets.size(0)\n",
        "                val_correct += (predicted == targets).sum().item()\n",
        "\n",
        "        val_acc = 100 * val_correct / val_total\n",
        "\n",
        "        scheduler.step(val_acc)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}] \"\n",
        "              f\"Train Acc: {train_acc:.2f}% \"\n",
        "              f\"Val Acc: {val_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "uQ4iikPc9qnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "Cp0059bQ933q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = extract_beats(train_records)\n",
        "X_test, y_test = extract_beats(test_records)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MULGfOJNVJ80",
        "outputId": "fed1f95d-bfcf-4ce0-80f0-9f2bca640869"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing record 101...\n",
            "Processing record 106...\n",
            "Processing record 108...\n",
            "Processing record 109...\n",
            "Processing record 112...\n",
            "Processing record 114...\n",
            "Processing record 115...\n",
            "Processing record 116...\n",
            "Processing record 118...\n",
            "Processing record 119...\n",
            "Processing record 122...\n",
            "Processing record 124...\n",
            "Processing record 201...\n",
            "Processing record 203...\n",
            "Processing record 205...\n",
            "Processing record 207...\n",
            "Processing record 208...\n",
            "Processing record 209...\n",
            "Processing record 215...\n",
            "Processing record 220...\n",
            "Processing record 223...\n",
            "Processing record 230...\n",
            "Processing record 100...\n",
            "Processing record 103...\n",
            "Processing record 105...\n",
            "Processing record 111...\n",
            "Processing record 113...\n",
            "Processing record 117...\n",
            "Processing record 121...\n",
            "Processing record 123...\n",
            "Processing record 200...\n",
            "Processing record 202...\n",
            "Processing record 210...\n",
            "Processing record 212...\n",
            "Processing record 213...\n",
            "Processing record 214...\n",
            "Processing record 219...\n",
            "Processing record 221...\n",
            "Processing record 222...\n",
            "Processing record 228...\n",
            "Processing record 231...\n",
            "Processing record 232...\n",
            "Processing record 233...\n",
            "Processing record 234...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(np.unique(y_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sp0yPnXEVTMT",
        "outputId": "db5e72b8-3382-4725-e7b5-8ff6bf1e9cc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 2 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, train_loader, test_loader, criterion, optimizer, scheduler, epochs=35)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "5g7V0Z3E9tVs",
        "outputId": "191a18ea-102a-46ca-da9a-faa38f8d5afa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/35] Train Acc: 76.55% Val Acc: 48.29%\n",
            "Epoch [2/35] Train Acc: 88.22% Val Acc: 65.10%\n",
            "Epoch [3/35] Train Acc: 91.06% Val Acc: 55.56%\n",
            "Epoch [4/35] Train Acc: 92.85% Val Acc: 70.31%\n",
            "Epoch [5/35] Train Acc: 93.50% Val Acc: 67.60%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3639108109.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m35\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-49601024.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, test_loader, criterion, optimizer, scheduler, epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m# ðŸ”¥ Gradient Clipping (NEW)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m             )\n\u001b[0;32m--> 630\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    365\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    863\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    866\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}